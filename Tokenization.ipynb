{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afdff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"A corpus is a collection of authentic text or audio organized into datasets.\" \\\n",
    "\"A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television \" \\\n",
    "\"shows, movies, and tweets. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2273abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "669960b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A corpus is a collection of authentic text or audio organized into datasets.A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "print(documents)\n",
    "print(type(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17830619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A corpus is a collection of authentic text or audio organized into datasets.A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7010b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'corpus', 'is', 'a', 'collection', 'of', 'authentic', 'text', 'or', 'audio', 'organized', 'into', 'datasets.A', 'corpus', 'can', 'be', 'made', 'up', 'of', 'everything', 'from', 'newspapers', ',', 'novels', ',', 'recipes', ',', 'radio', 'broadcasts', 'to', 'television', 'shows', ',', 'movies', ',', 'and', 'tweets', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "documents=word_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf08ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "corpus\n",
      "is\n",
      "a\n",
      "collection\n",
      "of\n",
      "authentic\n",
      "text\n",
      "or\n",
      "audio\n",
      "organized\n",
      "into\n",
      "datasets.A\n",
      "corpus\n",
      "can\n",
      "be\n",
      "made\n",
      "up\n",
      "of\n",
      "everything\n",
      "from\n",
      "newspapers\n",
      ",\n",
      "novels\n",
      ",\n",
      "recipes\n",
      ",\n",
      "radio\n",
      "broadcasts\n",
      "to\n",
      "television\n",
      "shows\n",
      ",\n",
      "movies\n",
      ",\n",
      "and\n",
      "tweets\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in documents:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fdcb24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'corpus',\n",
       " 'is',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'authentic',\n",
       " 'text',\n",
       " 'or',\n",
       " 'audio',\n",
       " 'organized',\n",
       " 'into',\n",
       " 'datasets',\n",
       " '.',\n",
       " 'A',\n",
       " 'corpus',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'up',\n",
       " 'of',\n",
       " 'everything',\n",
       " 'from',\n",
       " 'newspapers',\n",
       " ',',\n",
       " 'novels',\n",
       " ',',\n",
       " 'recipes',\n",
       " ',',\n",
       " 'radio',\n",
       " 'broadcasts',\n",
       " 'to',\n",
       " 'television',\n",
       " 'shows',\n",
       " ',',\n",
       " 'movies',\n",
       " ',',\n",
       " 'and',\n",
       " 'tweets',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize   #Separates punctuation from words\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7147f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'corpus',\n",
       " 'is',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'authentic',\n",
       " 'text',\n",
       " 'or',\n",
       " 'audio',\n",
       " 'organized',\n",
       " 'into',\n",
       " 'datasets.A',\n",
       " 'corpus',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'up',\n",
       " 'of',\n",
       " 'everything',\n",
       " 'from',\n",
       " 'newspapers',\n",
       " ',',\n",
       " 'novels',\n",
       " ',',\n",
       " 'recipes',\n",
       " ',',\n",
       " 'radio',\n",
       " 'broadcasts',\n",
       " 'to',\n",
       " 'television',\n",
       " 'shows',\n",
       " ',',\n",
       " 'movies',\n",
       " ',',\n",
       " 'and',\n",
       " 'tweets',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer   #Tokenizes words and separates punctuation from words\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
